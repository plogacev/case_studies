{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marketplace Paid Feature Pricing Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario Summary\n",
    "- The product we're pushing is boosts on the first 10 listings; we'll assume in the following that sellers upload or activate their listings in arbitrary order (i.e., regardless of which of the listings for that month would benefit the most from a boost)\n",
    "- We assume that the business is static, and doesn't change over time; no seasonality either "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavel/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the underscore notation for polars columns, similar to ibis and dplyr (i.e., `pl.col(\"column_name\")` -> `_.column_name`)\n",
    "class PolarsColumnNamespace:\n",
    "    def __getattr__(self, name):\n",
    "        return pl.col(name)\n",
    "\n",
    "# Enable _ as a shorthand for that class\n",
    "_ = PolarsColumnNamespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples from a beta distribution parameterized with mean and sd\n",
    "def sample_beta_mean_sd(mean, sd, size=None):\n",
    "    assert(0 < mean < 1)\n",
    "    var = sd ** 2\n",
    "    common = mean * (1 - mean) / (sd ** 2) - 1\n",
    "    alpha = mean * common\n",
    "    beta = (1 - mean) * common\n",
    "    if alpha <= 0 or beta <= 0:\n",
    "        raise ValueError(\"Invalid combination of mean and sd resulting in non-positive alpha/beta\")\n",
    "    return np.random.beta(alpha, beta, size=size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_lognormal_mean_sd(mean, sd, size=None):\n",
    "    sigma = np.sqrt(np.log(1 + (sd / mean) ** 2))\n",
    "    mu = np.log(mean) - 0.5 * sigma ** 2\n",
    "    return np.random.lognormal(mean=mu, sigma=sigma, size=size)\n",
    "\n",
    "def sample_lognormal_quantiles(q5, q95, size=None):\n",
    "    z5, z95 = -1.64485, 1.64485\n",
    "    sigma = (np.log(q95) - np.log(q5)) / (z95 - z5)\n",
    "    mu = np.log(q5) - sigma * z5\n",
    "    return np.random.lognormal(mean=mu, sigma=sigma, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit, logit\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def correlated_var(x, r):\n",
    "    \"\"\"\n",
    "    Generate a new variable correlated with x at level r while preserving\n",
    "    x's original mean and standard deviation.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    noise = np.random.normal(0, 1, n)\n",
    "\n",
    "    # Standardize x\n",
    "    z_x = zscore(x)\n",
    "\n",
    "    # Generate correlated variable in z-score space\n",
    "    y = r * z_x + np.sqrt(1 - r ** 2) * noise\n",
    "    \n",
    "    # Rescale z_y to match the original mean and std of x\n",
    "    return y * x.std() + x.mean()\n",
    "\n",
    "def correlated_rate(rate, corr_logit_space):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Convert actual boost effects to logit space\n",
    "    rate_logit = logit(rate)\n",
    "\n",
    "    # Create correlated perceived boost effect in logit space\n",
    "    rate_noisy_logit = correlated_var(rate_logit, r = corr_logit_space)\n",
    "\n",
    "    # Convert perceived boost effect back to probability space\n",
    "    rate_noisy = expit(rate_noisy_logit)\n",
    "\n",
    "    return rate_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "from scipy.special import expit, logit\n",
    "from scipy.stats import zscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceived_boost(actual_boost):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Convert actual boost effects to logit space\n",
    "    actual_boost_logit = logit(actual_boost)\n",
    "\n",
    "    # Create noisy perceived boost effect in logit space\n",
    "    actual_boost_noisy_logit = 1.25 * zscore(actual_boost_logit) * actual_boost_logit.std() + actual_boost_logit.mean() + np.random.normal(0, .2, len(actual_boost))\n",
    "\n",
    "    # Convert perceived boost effect back to probability space\n",
    "    return expit(actual_boost_noisy_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `par_` stands for parameter\n",
    "- `obs_` stands for observed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sellers(obs_months_active, min_months_active, max_months_active, avg_margin_rate, avg_boost_effect):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    seller_id = np.arange(n_sellers)\n",
    "    obs_months_active = np.random.randint(min_months_active, max_months_active + 1, size=n_sellers)\n",
    "\n",
    "    # Listings per month: 90% between 5-50\n",
    "    par_mean_active_listings_per_month = sample_lognormal_quantiles(q5 = 5, q95 = 50, size=n_sellers)\n",
    "\n",
    "    # Quantity sold per active listing per month: 90% between 3-30\n",
    "    par_mean_volume_per_listing_per_month = sample_lognormal_quantiles(q5 = 3, q95 = 30, size=n_sellers)\n",
    "\n",
    "    # Revenue: mean per unit sold, 90% between 20-500, sd as 30% of mean\n",
    "    par_mean_revenue_per_unit = sample_lognormal_quantiles(q5 = 20, q95 = 500, size=n_sellers)\n",
    "\n",
    "    # Margin by seller\n",
    "    par_margin_rate = sample_beta_mean_sd(avg_margin_rate, 0.15, size=n_sellers)\n",
    "\n",
    "    # Volume, revenue, and profit per month\n",
    "    par_mean_volume_per_month = par_mean_active_listings_per_month * par_mean_volume_per_listing_per_month\n",
    "    par_mean_revenue_per_month = par_mean_volume_per_month * par_mean_revenue_per_unit\n",
    "    par_mean_profit_per_month = par_mean_revenue_per_month * par_margin_rate\n",
    "    \n",
    "    # Profit per month for the first n listings\n",
    "    par_mean_profit_per_month_first_n_listings = par_margin_rate * par_mean_revenue_per_unit * par_mean_volume_per_month.clip(max=10)\n",
    "\n",
    "    # Boost effects rates: actual and perceived; assumed to be unrelated [to-do: make them correlated]\n",
    "    par_actual_boost_effect = sample_beta_mean_sd(mean = avg_boost_effect, sd = 0.075, size=n_sellers)\n",
    "    # Assume that people underestimate or overestimate the magnitude of the boost as it affects the business by various amounts\n",
    "    par_perceived_boost_effect = perceived_boost(par_actual_boost_effect)\n",
    "\n",
    "    # Boost effect: amounts, actual and perceived on the first up to 10 listings per month (that's equivalent to an arbitrary 10 listings under the current assumptions) \n",
    "    par_mean_actual_boost_amount_per_month = par_actual_boost_effect * par_mean_profit_per_month_first_n_listings\n",
    "    par_mean_perceived_boost_amount_per_month = par_perceived_boost_effect * par_mean_profit_per_month_first_n_listings\n",
    "\n",
    "\n",
    "    seller_data = pl.DataFrame({\n",
    "        'seller_id': seller_id,\n",
    "        'obs_months_active': obs_months_active,\n",
    "        \n",
    "        # Listings per month [→ Poisson λ]\n",
    "        'par_mean_active_listings_per_month': par_mean_active_listings_per_month,\n",
    "        \n",
    "        # Quantity sold per active listing per month [→ Poisson]\n",
    "        'par_mean_volume_per_listing_per_month': par_mean_volume_per_listing_per_month,\n",
    "\n",
    "        # Quantity sold per month [→ Poisson]\n",
    "        'par_mean_volume_per_month': par_mean_volume_per_month,\n",
    "\n",
    "        # Revenue per unit sold [→ Gaussian, or maybe Lognormal]\n",
    "        'par_mean_revenue_per_unit': par_mean_revenue_per_unit,\n",
    "        'par_sd_revenue_per_unit': par_mean_revenue_per_unit * 0.3,\n",
    "\n",
    "        # Boost effect: rates, actual and perceived; assumed to be unrelated\n",
    "        'par_actual_boost_effect': par_actual_boost_effect,\n",
    "        'par_perceived_boost_effect': par_perceived_boost_effect,\n",
    "        \n",
    "        # Revenue per month\n",
    "        'par_mean_revenue_per_month': par_mean_revenue_per_month,\n",
    "        \n",
    "        # Margin by seller\n",
    "        'par_margin_rate': par_margin_rate,\n",
    "        \n",
    "        # Profit per month\n",
    "        'par_mean_profit_per_month': par_mean_profit_per_month,\n",
    "        'par_mean_profit_per_month_first_n_listings': par_mean_profit_per_month,\n",
    "        \n",
    "        # Boost effect: amounts, actual and perceived\n",
    "        'par_mean_actual_boost_amount_per_month': par_mean_actual_boost_amount_per_month,\n",
    "        'par_mean_perceived_boost_amount_per_month': par_mean_perceived_boost_amount_per_month\n",
    "\n",
    "    })\n",
    "    \n",
    "    return seller_data\n",
    "\n",
    "\n",
    "def approximate_profit_per_month_distribution_first_n(n_inter, max_listings, par_mean_active_listings_per_month, par_mean_volume_per_listing_per_month, par_mean_revenue_per_unit, par_sd_revenue_per_unit, par_margin_rate, par_actual_boost_effect):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    sim_active_listings_per_month = np.random.poisson(par_mean_active_listings_per_month, size=n_inter)\n",
    " \n",
    "    for i in range(n_inter):\n",
    "        n_relevant_listings_per_month = sim_active_listings_per_month[i].clip(max=max_listings)\n",
    "        sim_volume_per_listing_per_month = [np.random.poisson(rate, size=n_months).sum() for rate, n_months in par_active_listings_per_month]\n",
    "        \n",
    "        np.random.poisson(par_mean_volume_per_listing_per_month, size=n_relevant_listings_per_month)\n",
    "        \n",
    "        par_mean_volume_per_listing_per_month\n",
    "        par_mean_revenue_per_unit, par_sd_revenue_per_unit,\n",
    "        par_margin_rate,\n",
    "        par_actual_boost_effect\n",
    "\n",
    "\n",
    "def sample_seller(obs_months_active, par_mean_active_listings_per_month, par_mean_volume_per_listing_per_month, par_mean_revenue_per_unit, par_sd_revenue_per_unit):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    par_active_listings_per_month = zip(par_mean_active_listings_per_month, obs_months_active)\n",
    "    obs_active_listing_month_pairs = [np.random.poisson(rate, size=n_months).sum() for rate, n_months in par_active_listings_per_month]\n",
    "    obs_active_listings_per_month = pl.Series(obs_active_listing_month_pairs) / obs_months_active\n",
    "\n",
    "    par_volume_per_listing_per_month = zip(par_mean_volume_per_listing_per_month, obs_active_listing_month_pairs)\n",
    "    obs_volume_total = [np.random.poisson(rate, size=n_listing_month_pairs).sum() for rate, n_listing_month_pairs in par_volume_per_listing_per_month]\n",
    "    obs_volume_per_month = pl.Series(obs_volume_total) / obs_months_active\n",
    "\n",
    "    par_revenue = zip(par_mean_revenue_per_unit, par_sd_revenue_per_unit, obs_volume_total)\n",
    "    obs_revenue_total = [sample_lognormal_mean_sd(mean, sd, size=n_volume_total).sum() for mean, sd, n_volume_total in par_revenue]\n",
    "    obs_revenue_per_month = pl.Series(obs_revenue_total) / obs_months_active\n",
    "\n",
    "    return pl.DataFrame({\n",
    "        'obs_listings_per_month': obs_active_listings_per_month,\n",
    "        'obs_volume_per_month': obs_volume_per_month,\n",
    "        'obs_revenue_per_month': obs_revenue_per_month\n",
    "    })\n",
    "    \n",
    "def sample_sellers(seller_data):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    obs_samples = seller_data.pipe(lambda s: sample_seller(\n",
    "        s[\"obs_months_active\"],\n",
    "        s[\"par_mean_active_listings_per_month\"],\n",
    "        s[\"par_mean_volume_per_listing_per_month\"],\n",
    "        s[\"par_mean_revenue_per_unit\"],\n",
    "        s[\"par_sd_revenue_per_unit\"]\n",
    "    ))\n",
    "    return pl.concat([seller_data, obs_samples], how=\"horizontal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10_000, 17)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>seller_id</th><th>obs_months_active</th><th>par_mean_active_listings_per_month</th><th>par_mean_volume_per_listing_per_month</th><th>par_mean_volume_per_month</th><th>par_mean_revenue_per_unit</th><th>par_sd_revenue_per_unit</th><th>par_actual_boost_effect</th><th>par_perceived_boost_effect</th><th>par_mean_revenue_per_month</th><th>par_margin_rate</th><th>par_mean_profit_per_month</th><th>par_mean_actual_boost_amount_per_month</th><th>par_mean_perceived_boost_amount_per_month</th><th>obs_listings_per_month</th><th>obs_volume_per_month</th><th>obs_revenue_per_month</th></tr><tr><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>41</td><td>8.67</td><td>22.06</td><td>191.25</td><td>94.1</td><td>28.23</td><td>0.03</td><td>0.02</td><td>17997.82</td><td>0.01</td><td>127.99</td><td>0.19</td><td>0.12</td><td>9.44</td><td>204.59</td><td>19257.25</td></tr><tr><td>1</td><td>31</td><td>22.27</td><td>5.27</td><td>117.27</td><td>105.7</td><td>31.71</td><td>0.14</td><td>0.2</td><td>12395.35</td><td>0.05</td><td>651.71</td><td>7.75</td><td>11.26</td><td>21.61</td><td>114.35</td><td>12052.79</td></tr><tr><td>2</td><td>17</td><td>13.63</td><td>12.87</td><td>175.42</td><td>139.22</td><td>41.77</td><td>0.04</td><td>0.04</td><td>24421.76</td><td>0.01</td><td>262.22</td><td>0.67</td><td>0.6</td><td>12.41</td><td>157.0</td><td>21603.87</td></tr><tr><td>3</td><td>45</td><td>29.25</td><td>7.84</td><td>229.27</td><td>23.19</td><td>6.96</td><td>0.12</td><td>0.12</td><td>5316.09</td><td>0.04</td><td>188.18</td><td>0.95</td><td>0.95</td><td>29.6</td><td>235.78</td><td>5477.44</td></tr><tr><td>4</td><td>10</td><td>56.46</td><td>26.35</td><td>1487.66</td><td>87.87</td><td>26.36</td><td>0.12</td><td>0.13</td><td>130716.21</td><td>0.66</td><td>86700.53</td><td>70.77</td><td>77.71</td><td>58.0</td><td>1544.9</td><td>135857.17</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>9995</td><td>18</td><td>12.91</td><td>4.41</td><td>56.9</td><td>33.68</td><td>10.1</td><td>0.22</td><td>0.28</td><td>1916.44</td><td>0.03</td><td>56.6</td><td>2.22</td><td>2.75</td><td>12.56</td><td>52.83</td><td>1792.79</td></tr><tr><td>9996</td><td>47</td><td>7.59</td><td>22.87</td><td>173.65</td><td>128.18</td><td>38.46</td><td>0.02</td><td>0.01</td><td>22258.85</td><td>0.08</td><td>1750.8</td><td>2.07</td><td>1.42</td><td>7.72</td><td>176.66</td><td>22570.05</td></tr><tr><td>9997</td><td>17</td><td>10.63</td><td>5.58</td><td>59.31</td><td>28.02</td><td>8.41</td><td>0.19</td><td>0.23</td><td>1661.68</td><td>0.06</td><td>105.01</td><td>3.38</td><td>4.1</td><td>10.59</td><td>57.88</td><td>1617.01</td></tr><tr><td>9998</td><td>3</td><td>7.99</td><td>8.91</td><td>71.25</td><td>164.45</td><td>49.34</td><td>0.07</td><td>0.1</td><td>11716.68</td><td>0.1</td><td>1133.11</td><td>11.8</td><td>15.79</td><td>6.67</td><td>66.0</td><td>10592.61</td></tr><tr><td>9999</td><td>24</td><td>35.64</td><td>10.35</td><td>368.99</td><td>495.48</td><td>148.64</td><td>0.06</td><td>0.05</td><td>182828.65</td><td>0.02</td><td>4245.55</td><td>7.24</td><td>6.27</td><td>36.17</td><td>376.29</td><td>186449.81</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10_000, 17)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ seller_id ┆ obs_month ┆ par_mean_ ┆ par_mean_ ┆ … ┆ par_mean_ ┆ obs_listi ┆ obs_volum ┆ obs_reve │\n",
       "│ ---       ┆ s_active  ┆ active_li ┆ volume_pe ┆   ┆ perceived ┆ ngs_per_m ┆ e_per_mon ┆ nue_per_ │\n",
       "│ i64       ┆ ---       ┆ stings_pe ┆ r_listing ┆   ┆ _boost_am ┆ onth      ┆ th        ┆ month    │\n",
       "│           ┆ i64       ┆ r_m…      ┆ _pe…      ┆   ┆ oun…      ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆           ┆ ---       ┆ ---       ┆   ┆ ---       ┆ f64       ┆ f64       ┆ f64      │\n",
       "│           ┆           ┆ f64       ┆ f64       ┆   ┆ f64       ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 0         ┆ 41        ┆ 8.67      ┆ 22.06     ┆ … ┆ 0.12      ┆ 9.44      ┆ 204.59    ┆ 19257.25 │\n",
       "│ 1         ┆ 31        ┆ 22.27     ┆ 5.27      ┆ … ┆ 11.26     ┆ 21.61     ┆ 114.35    ┆ 12052.79 │\n",
       "│ 2         ┆ 17        ┆ 13.63     ┆ 12.87     ┆ … ┆ 0.6       ┆ 12.41     ┆ 157.0     ┆ 21603.87 │\n",
       "│ 3         ┆ 45        ┆ 29.25     ┆ 7.84      ┆ … ┆ 0.95      ┆ 29.6      ┆ 235.78    ┆ 5477.44  │\n",
       "│ 4         ┆ 10        ┆ 56.46     ┆ 26.35     ┆ … ┆ 77.71     ┆ 58.0      ┆ 1544.9    ┆ 135857.1 │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 7        │\n",
       "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       "│ 9995      ┆ 18        ┆ 12.91     ┆ 4.41      ┆ … ┆ 2.75      ┆ 12.56     ┆ 52.83     ┆ 1792.79  │\n",
       "│ 9996      ┆ 47        ┆ 7.59      ┆ 22.87     ┆ … ┆ 1.42      ┆ 7.72      ┆ 176.66    ┆ 22570.05 │\n",
       "│ 9997      ┆ 17        ┆ 10.63     ┆ 5.58      ┆ … ┆ 4.1       ┆ 10.59     ┆ 57.88     ┆ 1617.01  │\n",
       "│ 9998      ┆ 3         ┆ 7.99      ┆ 8.91      ┆ … ┆ 15.79     ┆ 6.67      ┆ 66.0      ┆ 10592.61 │\n",
       "│ 9999      ┆ 24        ┆ 35.64     ┆ 10.35     ┆ … ┆ 6.27      ┆ 36.17     ┆ 376.29    ┆ 186449.8 │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 1        │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Synthetic Data Generation\n",
    "np.random.seed(42)\n",
    "n_sellers = 10000\n",
    "\n",
    "# User months active parameters\n",
    "min_months_active = 3\n",
    "max_months_active = 4 * 12\n",
    "\n",
    "# Assume that the average margin is 10%; of course, it varies by seller - somewhat more realistically, we could specify one by listing category\n",
    "avg_margin_rate = 0.15\n",
    "\n",
    "# Assume that the average boost effect on revenue is +15%; of course here, too there must be lots of variance by category and individually\n",
    "avg_boost_effect = 0.1\n",
    "\n",
    "# Generate the seller parameters\n",
    "seller_data = generate_sellers(n_sellers, min_months_active, max_months_active, avg_margin_rate, avg_boost_effect)\n",
    "\n",
    "# Sample the seller observations\n",
    "seller_data = sample_sellers(seller_data)\n",
    "\n",
    "# Round to 1 decimal place for printing\n",
    "seller_data.with_columns([\n",
    "    pl.col(col).round(2).alias(col) \n",
    "    for col, dtype in zip(seller_data.columns, seller_data.dtypes)\n",
    "    if dtype == pl.Float64\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the objective expected value of the package to the seller\n",
    "objective_expected_value = seller_data['θ_actual_boost_amount_per_month']\n",
    "\n",
    "# determine the subjective expected value of the package to the seller\n",
    "subjective_expected_value = seller_data['θ_perceived_boost_amount_per_month']\n",
    "\n",
    "# determine the willingness to pay based on the subjective expected value\n",
    "seller_data['willingness_to_pay'] = subjective_expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10404.0636"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "86700.53 * 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Assume sellers with more listings and higher item price gain more value\n",
    "#seller_data['potential_value'] = (\n",
    "#    0.5 * seller_data['listings'] + 0.05 * seller_data['avg_item_price'] + 2 * seller_data['past_promotions']\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Assume sellers with more listings and higher item price gain more value\n",
    "#seller_data['potential_value'] = (\n",
    "#    0.5 * seller_data['listings'] + 0.05 * seller_data['avg_item_price'] + 2 * seller_data['past_promotions']\n",
    "#)\n",
    "\n",
    "## True WTP: linear in potential value + noise\n",
    "#seller_data['true_wtp'] = seller_data['potential_value'] + np.random.normal(0, 2, n_sellers)\n",
    "\n",
    "## ## Experiment Simulation\n",
    "#price_points = np.array([5, 10, 15, 20, 25, 30])\n",
    "#assigned_prices = np.random.choice(price_points, n_sellers)\n",
    "#seller_data['price_offer'] = assigned_prices\n",
    "\n",
    "## Purchase probability drops sigmoidally with price vs. WTP\n",
    "#def purchase_prob(wtp, price):\n",
    "#    return 1 / (1 + np.exp(price - wtp))\n",
    "\n",
    "#seller_data['purchase_prob'] = purchase_prob(seller_data['true_wtp'], seller_data['price_offer'])\n",
    "#seller_data['purchase'] = np.random.binomial(1, seller_data['purchase_prob'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Willingness to Pay (WTP) using Bayesian Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    alpha = pm.Normal('alpha', 0, 5)\n",
    "    beta_price = pm.Normal('beta_price', 0, 5)\n",
    "    beta_listings = pm.Normal('beta_listings', 0, 1)\n",
    "    beta_item_price = pm.Normal('beta_item_price', 0, 0.1)\n",
    "    beta_past_promos = pm.Normal('beta_past_promos', 0, 1)\n",
    "\n",
    "    mu = (\n",
    "        alpha\n",
    "        + beta_price * seller_data['price_offer']\n",
    "        + beta_listings * seller_data['listings']\n",
    "        + beta_item_price * seller_data['avg_item_price']\n",
    "        + beta_past_promos * seller_data['past_promotions']\n",
    "    )\n",
    "\n",
    "    p = pm.math.sigmoid(mu)\n",
    "    purchase_obs = pm.Bernoulli('purchase_obs', p=p, observed=seller_data['purchase'])\n",
    "\n",
    "    trace = pm.sample(1000, tune=1000, target_accept=0.9, cores=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace, hdi_prob=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Optimal Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_means = trace.posterior.mean(dim=(\"chain\", \"draw\"))\n",
    "\n",
    "alpha_est = posterior_means['alpha'].values.item()\n",
    "beta_price_est = posterior_means['beta_price'].values.item()\n",
    "beta_listings_est = posterior_means['beta_listings'].values.item()\n",
    "beta_item_price_est = posterior_means['beta_item_price'].values.item()\n",
    "beta_past_promos_est = posterior_means['beta_past_promos'].values.item()\n",
    "\n",
    "def predict_prob(price, listings, avg_item_price, past_promotions):\n",
    "    mu = (\n",
    "        alpha_est\n",
    "        + beta_price_est * price\n",
    "        + beta_listings_est * listings\n",
    "        + beta_item_price_est * avg_item_price\n",
    "        + beta_past_promos_est * past_promotions\n",
    "    )\n",
    "    return 1 / (1 + np.exp(-mu))\n",
    "\n",
    "# Predict take rate and revenue at each price point for a sample seller\n",
    "sample_seller = seller_data.iloc[0]\n",
    "price_grid = np.linspace(5, 30, 50)\n",
    "probs = [\n",
    "    predict_prob(price, sample_seller['listings'], sample_seller['avg_item_price'], sample_seller['past_promotions'])\n",
    "    for price in price_grid\n",
    "]\n",
    "revenue = price_grid * np.array(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(price_grid, revenue)\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Expected Revenue')\n",
    "plt.title('Optimal Pricing Curve for Sample Seller')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Appendix / TODO\n",
    "# - Explore heterogeneous treatment effects: model interactions with seller features\n",
    "# - Segment sellers by value and compute personalized price points\n",
    "# - Add auction dynamics for limited promoted slots\n",
    "# - Simulate market competition effects\n",
    "# - Implement doubly robust causal estimators to correct for potential selection bias\n",
    "# - Build a dashboard/report with MyST or Voila for stakeholder presentation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
